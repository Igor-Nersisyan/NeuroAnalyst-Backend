# ==========================================================
#  NeuroAnalyst Backend ‚Äî Production
# ==========================================================

import os, re, json, time, uuid, logging
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import tldextract
import requests
from openai import OpenAI


# -------------------------------
# ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# -------------------------------
# –§–ò–ö–°: –ò—Å–ø–æ–ª—å–∑—É–µ–º export?format=txt –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ Google Docs
MAIN_PROMPT_URL = "https://docs.google.com/document/d/1DtA6CzcNeoZSDwj043YmE84XMnv1LAp_Z3MWxP8n55M/export?format=txt"
FOLLOWUP_PROMPT_URL = "https://docs.google.com/document/d/12nwxCLf4Gk4daR7ecRA04rZe-RToNb8-TAtERzY4o0E/export?format=txt"

SESSION_TTL_HOURS = 24
MAX_SESSIONS = 100

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("neuro-analyst")


# -------------------------------
# üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ Google Docs
# -------------------------------
def fetch_gdoc_text(gdoc_url: str) -> str:
    logger.info(f"üìÑ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É Google Doc: {gdoc_url}")
    try:
        r = requests.get(gdoc_url, timeout=30)
        logger.info(f"üìÑ –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {r.status_code}")
        if r.status_code != 200:
            logger.error(f"‚ùå Google Doc –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {r.status_code}")
            logger.error(f"‚ùå –û—Ç–≤–µ—Ç: {r.text[:500]}")
            raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Google Doc: {r.status_code}")
        text = r.text.strip()
        logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω Google Doc ({len(text):,} —Å–∏–º–≤–æ–ª–æ–≤)")
        if len(text) < 100:
            logger.warning(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç: {text[:100]}")
        return text
    except requests.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Google Doc: {e}")
        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")


# -------------------------------
# üåê –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫
# -------------------------------
def normalize_link(base, href: str):
    if not href or not isinstance(href, str):
        return None

    href = href.strip()

    bad_prefixes = (
        "mailto:", "tel:", "javascript:", "whatsapp:", "viber:",
        "tg:", "#", "sms:", "skype:",
    )
    if href.startswith(bad_prefixes):
        return None

    if href.startswith("http://") or href.startswith("https://"):
        return href.split("#")[0]

    if href.startswith("//"):
        return "https:" + href.split("#")[0]

    return urljoin(base, href.split("#")[0])


# -------------------------------
# üîé –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞
# -------------------------------
def same_domain(a, b):
    try:
        return tldextract.extract(a).registered_domain == tldextract.extract(b).registered_domain
    except:
        return False


def safe_json(obj):
    try:
        json.dumps(obj)
        return obj
    except:
        return str(obj)


def crawl_site(start_url, max_pages=25, depth=1):
    logger.info(f"üîé –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥: {start_url}")
    logger.info(f"üîé –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: max_pages={max_pages}, depth={depth}")
    
    visited, queue = set(), [(start_url, 0)]
    pages = []

    while queue and len(pages) < max_pages:
        url, d = queue.pop(0)
        if url in visited or d > depth:
            continue

        visited.add(url)
        logger.info(f"üåê [{len(pages)+1}/{max_pages}]: {url}")

        try:
            logger.info(f"üåê –î–µ–ª–∞—é –∑–∞–ø—Ä–æ—Å –∫ {url}...")
            r = requests.get(url, timeout=30, headers={"User-Agent": "NeuroAnalystBot/1.0"})
            logger.info(f"üåê –°—Ç–∞—Ç—É—Å: {r.status_code}")
            
            if r.status_code != 200:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é {url}: —Å—Ç–∞—Ç—É—Å {r.status_code}")
                continue

            logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ HTML...")
            soup = BeautifulSoup(r.text, "html.parser")
            for s in soup(["script", "style", "noscript"]):
                s.extract()

            title = soup.title.string.strip() if soup.title else ""
            text = soup.get_text("\n", strip=True)[:20000]
            logger.info(f"üåê –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")

            meta = {
                m.get("name", m.get("property", "")): m.get("content", "")
                for m in soup.find_all("meta")
                if m.get("name") or m.get("property")
            }

            links = []
            for a in soup.find_all("a", href=True):
                link = normalize_link(url, a["href"])
                if link and same_domain(start_url, link):
                    links.append(link)

            logger.info(f"üåê –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")
            
            pages.append({
                "url": url,
                "title": title,
                "meta": safe_json(meta),
                "text": text,
                "links": links
            })

            for l in links:
                if l not in visited:
                    queue.append((l, d + 1))

        except requests.Timeout:
            logger.error(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
        except requests.RequestException as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –¥–ª—è {url}: {e}")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}", exc_info=True)

    logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–±—Ä–∞–Ω–æ {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")
    return {"start_url": start_url, "pages": pages, "count": len(pages)}


# -------------------------------
# ü§ñ –ú–æ–¥–µ–ª–∏
# -------------------------------
def call_main_model(client, prompt_text, site_data):
    logger.info("ü§ñ –ó–∞–ø—Ä–æ—Å –∫ gpt-5-mini...")
    logger.info(f"ü§ñ –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(prompt_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    logger.info(f"ü§ñ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ site_data: {site_data.get('count', 0)}")
    
    messages = [
        {"role": "system", "content": prompt_text},
        {"role": "user", "content": json.dumps({"site": site_data}, ensure_ascii=False)},
    ]
    
    total_chars = len(prompt_text) + len(json.dumps(site_data))
    logger.info(f"ü§ñ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞: {total_chars:,} —Å–∏–º–≤–æ–ª–æ–≤")
    
    try:
        logger.info("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ OpenAI...")
        # –ë–ï–ó –¢–ê–ô–ú–ê–£–¢–û–í - –ø—É—Å—Ç—å –∂–¥–µ—Ç —Å–∫–æ–ª—å–∫–æ –Ω–∞–¥–æ
        resp = client.chat.completions.create(model="gpt-5-mini", messages=messages)
        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç gpt-5-mini")
        logger.info(f"‚úÖ –¢–æ–∫–µ–Ω—ã: {resp.usage.total_tokens:,} (prompt: {resp.usage.prompt_tokens:,}, completion: {resp.usage.completion_tokens:,})")
        return resp
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ gpt-5-mini: {e}", exc_info=True)
        raise


def call_followup_model(client, followup_prompt_text, json_payload):
    logger.info("üí¨ Follow-up –∑–∞–ø—Ä–æ—Å...")
    logger.info(f"üí¨ –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(followup_prompt_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    logger.info(f"üí¨ User instruction: {json_payload.get('user_instruction', '–ù–ï–¢')[:100]}")
    
    messages = [
        {"role": "system", "content": followup_prompt_text},
        {"role": "user", "content": json.dumps(json_payload, ensure_ascii=False)},
    ]
    
    total_chars = len(followup_prompt_text) + len(json.dumps(json_payload))
    logger.info(f"üí¨ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞: {total_chars:,} —Å–∏–º–≤–æ–ª–æ–≤")
    
    try:
        logger.info("üí¨ –û—Ç–ø—Ä–∞–≤–ª—è—é follow-up –∑–∞–ø—Ä–æ—Å –∫ OpenAI...")
        # –ë–ï–ó –¢–ê–ô–ú–ê–£–¢–û–í - –ø—É—Å—Ç—å –∂–¥–µ—Ç —Å–∫–æ–ª—å–∫–æ –Ω–∞–¥–æ
        resp = client.chat.completions.create(model="gpt-5-mini", messages=messages)
        logger.info(f"‚úÖ Follow-up –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç gpt-5-mini")
        logger.info(f"‚úÖ –¢–æ–∫–µ–Ω—ã: {resp.usage.total_tokens:,} (prompt: {resp.usage.prompt_tokens:,}, completion: {resp.usage.completion_tokens:,})")
        return resp
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ follow-up gpt-5-mini: {e}", exc_info=True)
        raise


# -------------------------------
# üóÑÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏—è–º–∏
# -------------------------------
STORE = {}

def cleanup_old_sessions():
    now = datetime.now()
    to_delete = [
        sid for sid, sess in STORE.items()
        if sess.get("created_at") and (now - sess["created_at"]) > timedelta(hours=SESSION_TTL_HOURS)
    ]
    
    for sid in to_delete:
        del STORE[sid]
    
    if to_delete:
        logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {len(to_delete)} —Å—Ç–∞—Ä—ã—Ö —Å–µ—Å—Å–∏–π")


def limit_sessions():
    if len(STORE) > MAX_SESSIONS:
        sorted_sessions = sorted(STORE.items(), key=lambda x: x[1].get("created_at", datetime.min))
        to_delete = len(STORE) - MAX_SESSIONS
        
        for sid, _ in sorted_sessions[:to_delete]:
            del STORE[sid]
        
        logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {to_delete} —Å–µ—Å—Å–∏–π (–ª–∏–º–∏—Ç)")


# -------------------------------
# üåç Flask API
# -------------------------------
app = Flask(__name__)
CORS(app)

# –ü–æ–ª—É—á–∞–µ–º OpenAI client –∏–∑ env
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    logger.error("‚ùå OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è!")
    raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

logger.info(f"üîë OpenAI API key –∑–∞–≥—Ä—É–∂–µ–Ω (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–∏–º–≤–æ–ª–∞: ...{api_key[-4:]})")
OPENAI_CLIENT = OpenAI(api_key=api_key)
logger.info("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")


@app.route("/ping", methods=["GET"])
def ping():
    return jsonify({
        "status": "alive",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "sessions": len(STORE)
    }), 200


@app.route("/analyze", methods=["POST"])
def analyze():
    logger.info("=" * 60)
    logger.info("üÜï /analyze")
    
    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
    logger.info(f"Request data: {request.json}")
    logger.info(f"Headers: {dict(request.headers)}")
    
    cleanup_old_sessions()
    limit_sessions()
    
    data = request.json or {}
    site_url = data.get("site_url")
    existing_sid = data.get("session_id")
    
    logger.info(f"üìù site_url: {site_url}")
    logger.info(f"üìù existing_sid: {existing_sid}")

    if not site_url:
        logger.warning("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç site_url")
        return jsonify({"error": "–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å site_url"}), 400

    # –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º session_id –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if existing_sid and existing_sid in STORE:
        sid = existing_sid
        logger.info(f"‚ôªÔ∏è –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É—é session_id: {sid}")
    else:
        sid = str(uuid.uuid4())
        logger.info(f"üÜï –ù–æ–≤—ã–π session_id: {sid}")

    try:
        logger.info("üìÑ –®–∞–≥ 1: –ó–∞–≥—Ä—É–∂–∞—é –ø—Ä–æ–º–ø—Ç –∏–∑ Google Doc...")
        main_prompt = fetch_gdoc_text(MAIN_PROMPT_URL)
        logger.info(f"üìÑ –ü—Ä–æ–º–ø—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(main_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        logger.info("üåê –®–∞–≥ 2: –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞...")
        site_data = crawl_site(site_url)
        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {site_data['count']} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        logger.info("ü§ñ –®–∞–≥ 3: –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –≤ GPT...")
        resp = call_main_model(OPENAI_CLIENT, main_prompt, site_data)
        
        logger.info("ü§ñ –®–∞–≥ 4: –ò–∑–≤–ª–µ–∫–∞—é –æ—Ç–≤–µ—Ç...")
        model_output = resp.choices[0].message.content
        logger.info(f"ü§ñ –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(model_output)} —Å–∏–º–≤–æ–ª–æ–≤")

    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ /analyze: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

    # –ü–û–õ–ù–ê–Ø –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Å–µ—Å—Å–∏–∏
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è—é —Å–µ—Å—Å–∏—é...")
    STORE[sid] = {
        "site": site_data,
        "first_output": model_output,
        "last_followup": None,
        "history": [],
        "created_at": datetime.now()
    }
    logger.info(f"üíæ –°–µ—Å—Å–∏—è {sid} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

    logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
    logger.info("=" * 60)

    return jsonify({
        "session_id": sid,
        "result": model_output,
        "pages": site_data["count"]
    })


@app.route("/followup", methods=["POST"])
def followup():
    logger.info("=" * 60)
    logger.info("üí¨ /followup")
    
    data = request.json or {}
    sid = data.get("session_id")
    user_instruction = data.get("followup_prompt")
    
    logger.info(f"üìù session_id: {sid}")
    logger.info(f"üìù user_instruction: {user_instruction[:100] if user_instruction else '–ù–ï–¢'}")

    if not sid or sid not in STORE:
        logger.warning(f"‚ö†Ô∏è session_id {sid} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ STORE")
        logger.info(f"‚ö†Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å–µ—Å—Å–∏–∏: {list(STORE.keys())}")
        return jsonify({"error": "session_id –Ω–µ –Ω–∞–π–¥–µ–Ω"}), 404

    sess = STORE[sid]
    logger.info(f"üìÇ –°–µ—Å—Å–∏—è –Ω–∞–π–¥–µ–Ω–∞. –ò—Å—Ç–æ—Ä–∏—è: {len(sess.get('history', []))} —Å–æ–æ–±—â–µ–Ω–∏–π")

    try:
        logger.info("üìÑ –ó–∞–≥—Ä—É–∂–∞—é follow-up –ø—Ä–æ–º–ø—Ç...")
        followup_prompt_text = fetch_gdoc_text(FOLLOWUP_PROMPT_URL)
        logger.info(f"üìÑ Follow-up –ø—Ä–æ–º–ø—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(followup_prompt_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ follow-up –ø—Ä–æ–º–ø—Ç–∞: {e}", exc_info=True)
        return jsonify({"error": f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–∞: {e}"}), 500

    payload = {
        "first_output": sess.get("first_output"),
        "last_followup": sess.get("last_followup"),
        "conversation_history": sess.get("history", []),
        "user_instruction": user_instruction
    }
    
    logger.info(f"üì¶ –†–∞–∑–º–µ—Ä payload: {len(json.dumps(payload))} —Å–∏–º–≤–æ–ª–æ–≤")

    try:
        logger.info("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é follow-up –∑–∞–ø—Ä–æ—Å –≤ GPT...")
        resp = call_followup_model(OPENAI_CLIENT, followup_prompt_text, payload)
        
        logger.info("ü§ñ –ò–∑–≤–ª–µ–∫–∞—é –æ—Ç–≤–µ—Ç...")
        model_text = resp.choices[0].message.content
        logger.info(f"ü§ñ –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(model_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        logger.info("üíæ –û–±–Ω–æ–≤–ª—è—é —Å–µ—Å—Å–∏—é...")
        sess["last_followup"] = model_text
        sess["history"].append({"role": "user", "content": user_instruction})
        sess["history"].append({"role": "assistant", "content": model_text})
        logger.info(f"üíæ –ò—Å—Ç–æ—Ä–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(sess['history'])} —Å–æ–æ–±—â–µ–Ω–∏–π")

    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ /followup: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

    logger.info(f"‚úÖ Follow-up –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
    logger.info("=" * 60)

    return jsonify({"result": model_text})


@app.route("/clear-chat", methods=["POST"])
def clear_chat():
    data = request.json or {}
    sid = data.get("session_id")

    if not sid or sid not in STORE:
        return jsonify({"error": "session_id –Ω–µ –Ω–∞–π–¥–µ–Ω"}), 404

    sess = STORE[sid]
    messages_count = len(sess.get("history", []))
    
    sess["history"] = []
    sess["last_followup"] = None
    
    logger.info(f"üßπ –û—á–∏—â–µ–Ω —á–∞—Ç ({messages_count} —Å–æ–æ–±—â–µ–Ω–∏–π)")

    return jsonify({
        "status": "success",
        "message": f"–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –æ—á–∏—â–µ–Ω–∞ ({messages_count} —Å–æ–æ–±—â–µ–Ω–∏–π)",
        "session_id": sid
    }), 200


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    app.run(host="0.0.0.0", port=port)
